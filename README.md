# MyModernTransformer
My implementation of a more modern Decoder-Only Transformer model architecture which, in turn, implements RoPE, KV-Cache, RMS Norm, Grouped Query Attention, SwiGLU, etc. This implementation will forego training and instead utilize the LLaMA2 weights for inference. 
